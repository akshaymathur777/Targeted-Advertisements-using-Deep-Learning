Microsoft Windows [Version 10.0.17134.376]
(c) 2018 Microsoft Corporation. All rights reserved.

C:\Users\Akshay>cd Desktop

C:\Users\Akshay\Desktop>python train.py --dataset dataset --model e_75.model --labelbin e_75.pickle
C:\Users\Akshay\AppData\Local\Programs\Python\Python36\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
[INFO] loading images...
libpng warning: iCCP: known incorrect sRGB profile
Corrupt JPEG data: 1 extraneous bytes before marker 0xd9
[INFO] data matrix: 385.34MB
[INFO] compiling model...
2018-10-30 01:11:07.101285: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 96, 96, 32)        896
_________________________________________________________________
activation_1 (Activation)    (None, 96, 96, 32)        0
_________________________________________________________________
batch_normalization_1 (Batch (None, 96, 96, 32)        128
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 32, 32)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496
_________________________________________________________________
activation_2 (Activation)    (None, 32, 32, 64)        0
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 32, 64)        256
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 32, 64)        36928
_________________________________________________________________
activation_3 (Activation)    (None, 32, 32, 64)        0
_________________________________________________________________
batch_normalization_3 (Batch (None, 32, 32, 64)        256
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0
_________________________________________________________________
dropout_2 (Dropout)          (None, 16, 16, 64)        0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 16, 16, 128)       73856
_________________________________________________________________
activation_4 (Activation)    (None, 16, 16, 128)       0
_________________________________________________________________
batch_normalization_4 (Batch (None, 16, 16, 128)       512
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 16, 16, 128)       147584
_________________________________________________________________
activation_5 (Activation)    (None, 16, 16, 128)       0
_________________________________________________________________
batch_normalization_5 (Batch (None, 16, 16, 128)       512
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0
_________________________________________________________________
dropout_3 (Dropout)          (None, 8, 8, 128)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 8192)              0
_________________________________________________________________
dense_1 (Dense)              (None, 1024)              8389632
_________________________________________________________________
activation_6 (Activation)    (None, 1024)              0
_________________________________________________________________
batch_normalization_6 (Batch (None, 1024)              4096
_________________________________________________________________
dropout_4 (Dropout)          (None, 1024)              0
_________________________________________________________________
dense_2 (Dense)              (None, 3)                 3075
_________________________________________________________________
activation_7 (Activation)    (None, 3)                 0
=================================================================
Total params: 8,676,227
Trainable params: 8,673,347
Non-trainable params: 2,880
_________________________________________________________________
[INFO] training network...
WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.
Epoch 1/75
44/44 [==============================] - 223s 5s/step - loss: 1.9541 - acc: 0.4362 - val_loss: 1.9172 - val_acc: 0.4510
Epoch 2/75
44/44 [==============================] - 223s 5s/step - loss: 1.2578 - acc: 0.4909 - val_loss: 1.7162 - val_acc: 0.4678
Epoch 3/75
44/44 [==============================] - 222s 5s/step - loss: 1.1087 - acc: 0.5226 - val_loss: 1.4391 - val_acc: 0.4902
Epoch 4/75
44/44 [==============================] - 222s 5s/step - loss: 0.9967 - acc: 0.5657 - val_loss: 1.2404 - val_acc: 0.5462
Epoch 5/75
44/44 [==============================] - 217s 5s/step - loss: 0.9801 - acc: 0.5669 - val_loss: 1.5527 - val_acc: 0.4986
Epoch 6/75
44/44 [==============================] - 209s 5s/step - loss: 0.9680 - acc: 0.5761 - val_loss: 1.2513 - val_acc: 0.5294
Epoch 7/75
44/44 [==============================] - 212s 5s/step - loss: 0.9265 - acc: 0.5733 - val_loss: 1.3379 - val_acc: 0.5126
Epoch 8/75
44/44 [==============================] - 212s 5s/step - loss: 0.9514 - acc: 0.5816 - val_loss: 1.2408 - val_acc: 0.5742
Epoch 9/75
44/44 [==============================] - 220s 5s/step - loss: 0.9615 - acc: 0.5735 - val_loss: 0.9090 - val_acc: 0.6611
Epoch 10/75
44/44 [==============================] - 208s 5s/step - loss: 0.8822 - acc: 0.6254 - val_loss: 1.1217 - val_acc: 0.6246
Epoch 11/75
44/44 [==============================] - 191s 4s/step - loss: 0.8661 - acc: 0.6313 - val_loss: 0.7130 - val_acc: 0.7059
Epoch 12/75
44/44 [==============================] - 193s 4s/step - loss: 0.8579 - acc: 0.6240 - val_loss: 0.8107 - val_acc: 0.6387
Epoch 13/75
44/44 [==============================] - 191s 4s/step - loss: 0.8540 - acc: 0.6268 - val_loss: 0.8334 - val_acc: 0.6527
Epoch 14/75
44/44 [==============================] - 188s 4s/step - loss: 0.8576 - acc: 0.6271 - val_loss: 0.9604 - val_acc: 0.6218
Epoch 15/75
44/44 [==============================] - 191s 4s/step - loss: 0.8472 - acc: 0.6303 - val_loss: 0.8910 - val_acc: 0.6218
Epoch 16/75
44/44 [==============================] - 182s 4s/step - loss: 0.7521 - acc: 0.6569 - val_loss: 0.7261 - val_acc: 0.6779
Epoch 17/75
44/44 [==============================] - 187s 4s/step - loss: 0.7832 - acc: 0.6357 - val_loss: 0.7996 - val_acc: 0.6583
Epoch 18/75
44/44 [==============================] - 184s 4s/step - loss: 0.7767 - acc: 0.6543 - val_loss: 0.9954 - val_acc: 0.6078
Epoch 19/75
44/44 [==============================] - 183s 4s/step - loss: 0.7439 - acc: 0.6907 - val_loss: 0.7329 - val_acc: 0.6499
Epoch 20/75
44/44 [==============================] - 182s 4s/step - loss: 0.7338 - acc: 0.6869 - val_loss: 0.7691 - val_acc: 0.6751
Epoch 21/75
44/44 [==============================] - 183s 4s/step - loss: 0.7488 - acc: 0.6782 - val_loss: 0.6682 - val_acc: 0.7283
Epoch 22/75
44/44 [==============================] - 186s 4s/step - loss: 0.6758 - acc: 0.7038 - val_loss: 0.9046 - val_acc: 0.6359
Epoch 23/75
44/44 [==============================] - 184s 4s/step - loss: 0.6992 - acc: 0.6962 - val_loss: 0.7081 - val_acc: 0.7059
Epoch 24/75
44/44 [==============================] - 181s 4s/step - loss: 0.6329 - acc: 0.7181 - val_loss: 0.7337 - val_acc: 0.6639
Epoch 25/75
44/44 [==============================] - 183s 4s/step - loss: 0.7199 - acc: 0.6848 - val_loss: 0.8919 - val_acc: 0.6751
Epoch 26/75
44/44 [==============================] - 184s 4s/step - loss: 0.6526 - acc: 0.7188 - val_loss: 0.6615 - val_acc: 0.7171
Epoch 27/75
44/44 [==============================] - 183s 4s/step - loss: 0.6515 - acc: 0.7135 - val_loss: 0.6865 - val_acc: 0.7227
Epoch 28/75
44/44 [==============================] - 181s 4s/step - loss: 0.6282 - acc: 0.7203 - val_loss: 0.6299 - val_acc: 0.7255
Epoch 29/75
44/44 [==============================] - 183s 4s/step - loss: 0.6242 - acc: 0.7364 - val_loss: 0.9257 - val_acc: 0.6331
Epoch 30/75
44/44 [==============================] - 184s 4s/step - loss: 0.5693 - acc: 0.7479 - val_loss: 0.6917 - val_acc: 0.6723
Epoch 31/75
44/44 [==============================] - 182s 4s/step - loss: 0.6012 - acc: 0.7199 - val_loss: 0.6429 - val_acc: 0.7339
Epoch 32/75
44/44 [==============================] - 183s 4s/step - loss: 0.6083 - acc: 0.7390 - val_loss: 0.6950 - val_acc: 0.7199
Epoch 33/75
44/44 [==============================] - 182s 4s/step - loss: 0.6089 - acc: 0.7502 - val_loss: 1.0145 - val_acc: 0.6134
Epoch 34/75
44/44 [==============================] - 182s 4s/step - loss: 0.5800 - acc: 0.7372 - val_loss: 0.5898 - val_acc: 0.7591
Epoch 35/75
44/44 [==============================] - 182s 4s/step - loss: 0.5796 - acc: 0.7521 - val_loss: 0.6263 - val_acc: 0.7367
Epoch 36/75
44/44 [==============================] - 182s 4s/step - loss: 0.5458 - acc: 0.7649 - val_loss: 0.5621 - val_acc: 0.7423
Epoch 37/75
44/44 [==============================] - 182s 4s/step - loss: 0.5334 - acc: 0.7810 - val_loss: 0.8107 - val_acc: 0.6751
Epoch 38/75
44/44 [==============================] - 181s 4s/step - loss: 0.5662 - acc: 0.7702 - val_loss: 0.9062 - val_acc: 0.6779
Epoch 39/75
44/44 [==============================] - 184s 4s/step - loss: 0.5979 - acc: 0.7443 - val_loss: 0.6374 - val_acc: 0.7395
Epoch 40/75
44/44 [==============================] - 183s 4s/step - loss: 0.5334 - acc: 0.7679 - val_loss: 0.8484 - val_acc: 0.7031
Epoch 41/75
44/44 [==============================] - 183s 4s/step - loss: 0.5881 - acc: 0.7644 - val_loss: 1.4671 - val_acc: 0.5882
Epoch 42/75
44/44 [==============================] - 182s 4s/step - loss: 0.5759 - acc: 0.7585 - val_loss: 0.7829 - val_acc: 0.7003
Epoch 43/75
44/44 [==============================] - 182s 4s/step - loss: 0.5910 - acc: 0.7606 - val_loss: 1.1830 - val_acc: 0.6078
Epoch 44/75
44/44 [==============================] - 182s 4s/step - loss: 0.5243 - acc: 0.7739 - val_loss: 0.7478 - val_acc: 0.6975
Epoch 45/75
44/44 [==============================] - 182s 4s/step - loss: 0.5112 - acc: 0.7788 - val_loss: 0.7199 - val_acc: 0.7115
Epoch 46/75
44/44 [==============================] - 182s 4s/step - loss: 0.5025 - acc: 0.7921 - val_loss: 0.9052 - val_acc: 0.6835
Epoch 47/75
44/44 [==============================] - 182s 4s/step - loss: 0.5193 - acc: 0.7779 - val_loss: 0.8451 - val_acc: 0.6947
Epoch 48/75
44/44 [==============================] - 184s 4s/step - loss: 0.5082 - acc: 0.7848 - val_loss: 1.7986 - val_acc: 0.5098
Epoch 49/75
44/44 [==============================] - 183s 4s/step - loss: 0.5016 - acc: 0.7814 - val_loss: 0.5736 - val_acc: 0.7507
Epoch 50/75
44/44 [==============================] - 187s 4s/step - loss: 0.4920 - acc: 0.8065 - val_loss: 0.5871 - val_acc: 0.7535
Epoch 51/75
44/44 [==============================] - 184s 4s/step - loss: 0.4174 - acc: 0.8288 - val_loss: 0.8546 - val_acc: 0.6975
Epoch 52/75
44/44 [==============================] - 184s 4s/step - loss: 0.4199 - acc: 0.8380 - val_loss: 0.6416 - val_acc: 0.7507
Epoch 53/75
44/44 [==============================] - 185s 4s/step - loss: 0.4095 - acc: 0.8307 - val_loss: 0.5303 - val_acc: 0.7815
Epoch 54/75
44/44 [==============================] - 184s 4s/step - loss: 0.4400 - acc: 0.8295 - val_loss: 0.5852 - val_acc: 0.7619
Epoch 55/75
44/44 [==============================] - 180s 4s/step - loss: 0.4433 - acc: 0.8195 - val_loss: 1.4854 - val_acc: 0.6246
Epoch 56/75
44/44 [==============================] - 182s 4s/step - loss: 0.4132 - acc: 0.8380 - val_loss: 0.5085 - val_acc: 0.7899
Epoch 57/75
44/44 [==============================] - 182s 4s/step - loss: 0.4086 - acc: 0.8430 - val_loss: 0.6610 - val_acc: 0.7479
Epoch 58/75
44/44 [==============================] - 184s 4s/step - loss: 0.3703 - acc: 0.8516 - val_loss: 0.6427 - val_acc: 0.7591
Epoch 59/75
44/44 [==============================] - 181s 4s/step - loss: 0.4351 - acc: 0.8278 - val_loss: 0.6167 - val_acc: 0.7563
Epoch 60/75
44/44 [==============================] - 183s 4s/step - loss: 0.4000 - acc: 0.8423 - val_loss: 0.5598 - val_acc: 0.8123
Epoch 61/75
44/44 [==============================] - 182s 4s/step - loss: 0.3516 - acc: 0.8567 - val_loss: 0.5748 - val_acc: 0.7787
Epoch 62/75
44/44 [==============================] - 182s 4s/step - loss: 0.4126 - acc: 0.8392 - val_loss: 0.6259 - val_acc: 0.7871
Epoch 63/75
44/44 [==============================] - 182s 4s/step - loss: 0.4273 - acc: 0.8271 - val_loss: 0.7511 - val_acc: 0.7059
Epoch 64/75
44/44 [==============================] - 182s 4s/step - loss: 0.4153 - acc: 0.8399 - val_loss: 0.5622 - val_acc: 0.7703
Epoch 65/75
44/44 [==============================] - 184s 4s/step - loss: 0.4321 - acc: 0.8153 - val_loss: 0.8547 - val_acc: 0.7591
Epoch 66/75
44/44 [==============================] - 180s 4s/step - loss: 0.4349 - acc: 0.8235 - val_loss: 0.7145 - val_acc: 0.7339
Epoch 67/75
44/44 [==============================] - 183s 4s/step - loss: 0.4094 - acc: 0.8335 - val_loss: 0.5465 - val_acc: 0.7871
Epoch 68/75
44/44 [==============================] - 182s 4s/step - loss: 0.3758 - acc: 0.8418 - val_loss: 0.5934 - val_acc: 0.7619
Epoch 69/75
44/44 [==============================] - 182s 4s/step - loss: 0.4522 - acc: 0.8248 - val_loss: 0.7470 - val_acc: 0.7031
Epoch 70/75
44/44 [==============================] - 182s 4s/step - loss: 0.3960 - acc: 0.8513 - val_loss: 0.5562 - val_acc: 0.7759
Epoch 71/75
44/44 [==============================] - 184s 4s/step - loss: 0.3605 - acc: 0.8629 - val_loss: 0.6263 - val_acc: 0.7675
Epoch 72/75
44/44 [==============================] - 182s 4s/step - loss: 0.3169 - acc: 0.8754 - val_loss: 0.4835 - val_acc: 0.8011
Epoch 73/75
44/44 [==============================] - 180s 4s/step - loss: 0.3310 - acc: 0.8747 - val_loss: 0.5453 - val_acc: 0.7843
Epoch 74/75
44/44 [==============================] - 183s 4s/step - loss: 0.3085 - acc: 0.8793 - val_loss: 0.5359 - val_acc: 0.7843
Epoch 75/75
44/44 [==============================] - 180s 4s/step - loss: 0.3131 - acc: 0.8759 - val_loss: 0.5487 - val_acc: 0.7731
[INFO] serializing network...
[INFO] serializing label binarizer...
14197.24975156784

C:\Users\Akshay\Desktop>